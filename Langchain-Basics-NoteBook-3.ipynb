{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13bab812-9a26-48f8-9d03-27e32d02d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import numpy as np\n",
    "from langchain_together import Together\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac871e2-877f-47bb-bb3b-afa5e757cf30",
   "metadata": {},
   "source": [
    "# Pdf Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131a0ad1-3870-4892-8822-5a13ad9c0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# for multiple pdf files\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"MachineLearning-Lecture03.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ea0b3c-05d2-48f5-92fb-120bb48a3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb1efd-6fa4-461d-b05e-5239a211a136",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2cd620e-b360-4409-99d5-e36ec91571b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "# embeddings = OllamaEmbeddings()\n",
    "\n",
    "from langchain_together.embeddings import TogetherEmbeddings\n",
    "embeddings = TogetherEmbeddings(\n",
    "    model=\"togethercomputer/m2-bert-80M-8k-retrieval\",\n",
    "    together_api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\"                          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd43dc83-6ae7-48ce-b5a3-2b4f324d60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"dogs are good\"\n",
    "text2 = \"dogs are better\"\n",
    "text3 = \"israel is a country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2cfa3419-4e7a-4fd2-917d-acd8942327b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_1 = embeddings.embed_query(text1)\n",
    "emb_2 = embeddings.embed_query(text2)\n",
    "emb_3 = embeddings.embed_query(text3)\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "cosine_similarity_1 = 1 - cosine(emb_1, emb_2)\n",
    "cosine_similarity_2 = 1 - cosine(emb_1, emb_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d70279a-1da1-4376-b8d6-ad1683f50588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28266973618505165"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6af422-2e98-424a-b261-04acf9ffec76",
   "metadata": {},
   "source": [
    "# VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdd2c3-d656-4fe9-9cf8-4c02803fe7aa",
   "metadata": {},
   "source": [
    "FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8d75085-7e44-4e39-9537-6e7e9c019884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bac9e4e0-c0d9-4b8f-9461-8b49d4015b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2d719a3-5933-4c77-84fc-a1c8daa7ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'docs/faiss/'\n",
    "vectordb.save_local(folder_path=folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed57500d-b202-44e5-ba30-e33d4b48e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings()\n",
    "loaded_vectordb = FAISS.load_local(folder_path=folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff57257-48a8-4ab0-a1fc-42a47f62a263",
   "metadata": {},
   "source": [
    "Chroma Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e07bb4b2-900e-4040-8aa8-c5ca34856f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "# persist_directory = 'docs/chroma/file_n'\n",
    "# persist_directory = 'docs/chroma/vector-kaggle'\n",
    "persist_directory = 'docs/chroma/CSLectures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c91fff52-708c-4544-9f72-48779e7270f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=persist_directory\n",
    "# )\n",
    "# vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92c19bf3-5122-400f-83f9-33221fd81990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing from local save\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e33c783-6cae-4032-ac2e-4e28df0c94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8924eec-af9a-44f8-bdce-6c3151ac80d0",
   "metadata": {},
   "source": [
    "# Similarity_Search + Max_Marginal_Relevance_Search\n",
    "``Similarity-search`` prioritizes finding the closest documents\n",
    "\n",
    "``MMR-search`` focuses on retrieving a set of documents that are both relevant and diverse, covering different aspects of the query\n",
    "\n",
    "**Similarity Search**: Use this when you simply want the most similar documents to the query, regardless of their redundancy. It's faster and easier to implement.\n",
    "\n",
    "**Max-Marginal Relevance Search**: Use this when you need a diverse set of documents that cover different aspects of the query. This is particularly helpful for tasks like document summarization or generating informative responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acb3c406-564c-4470-a6a3-e6ba2b9f0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is Convolutional Neural Networks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "532de458-dc68-4e47-b531-449b581c082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0aa75103-c6a8-4072-b0ab-151f97c4ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying which file to retrieve info from\n",
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=2,\n",
    "    filter={\"source\":\"file_1.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a144e15-e7da-4219-acbe-1c63d6d6be12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='3. Expand clusters from core points.  \\n• Strengths : Can find arbitrarily shaped clusters, robust to noise and outliers.  \\n• Weaknesses : Not suitable for datasets with varying densities, sensitive to parameter \\nselection.  \\nDimensionality Reduction  \\nDimensionality reduction involves reducing the number of random variables under \\nconsideration, making the data easier to visualize and often improving algorithm performance.  \\nPrincipal Component Analysis (PCA)  \\n• Concept : PCA transforms the data to a new coordinate system where the greatest \\nvariance lies on the first axis, the second greatest variance on the second axis, and so on.  \\n• Algorithm : \\n1. Standardize the data.  \\n2. Calculate the covariance matrix.  \\n3. Calculate the eigenvalues and eigenvectors of the covariance matrix.  \\n4. Sort eigenvalues and eigenvectors.  \\n5. Select the top k eigenvectors to form a new feature space.  \\n• Strengths : Reduces complexity, improves computational efficiency, captures the most \\nimportant variance in the data.  \\n• Weaknesses : Linear method, may not capture complex structures, sensitive to outliers.  \\nt-Distributed Stochastic Neighbor Embedding (t -SNE)  \\n• Concept : t-SNE is a non -linear dimensionality reduction technique that is particularly \\nwell-suited for visualizing high -dimensional data in 2 or 3 dimensions.  \\n• Algorithm : \\n1. Compute pairwise similarities between points in the high-dimensional space.  \\n2. Define a probability distribution over pairs of points.  \\n3. Define a similar distribution in the lower -dimensional space.  \\n4. Minimize the Kullback -Leibler divergence between the two distributions.  \\n• Strengths : Excellent for visualization, preserves local structure.  \\n• Weaknesses : Computationally intensive, difficult to interpret, sensitive to \\nhyperparameters.  \\nLinear Discriminant Analysis (LDA)  \\n• Concept : LDA is a supervised dimensionality reduction technique that aims to find a \\nlinear combination of features that best separate two or more classes.  \\n• Algorithm : \\n1. Compute the within -class and between -class scatter matrices.  \\n2. Compute the eigenvalues and eigenvectors of the scatter matrices.  \\n3. Select the top eigenvectors to form a new feature space.  \\n• Strengths : Effective for classification tasks, maximizes class separability.  ', metadata={'page': 1, 'source': 'file_2.pdf'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e8814b-3ad9-4507-b20f-4afbcd658f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='• Strengths : Capable of generating high -quality synthetic data, useful for image \\ngeneration, style transfer, and data augmentation.  \\n• Weaknesses : Difficult to train, prone to mode collapse where the generator produces \\nlimited varieties of outputs.  \\nAutoencoders  \\n• Concept : Autoencoders are neural networks used to learn efficient codings of input data, \\ntypically for the purposes of dimensionality reduction or feature learning.  \\n• Architecture : \\no Encoder : Compresses the input data into a latent -space representation.  \\no Decoder : Reconstructs the input data from the latent representation.  \\no Variational Autoencoders (VAEs) : A type of autoencoder that provides a \\nprobabilistic manner for describing an observation in latent space.  \\n• Strengths : Useful for noise reduction, data compression, and feature extraction.  \\n• Weaknesses : Reconstructed data might not be perfect, requires careful tuning of \\narchitecture and hyperparameters.  \\nTransformer Models (e.g., BERT, GPT)  \\n• Concept : Transformers are a type of neural network architecture designed to handle \\nsequential data, replacing traditional RNNs by using self -attention mechanisms.  \\n• Architecture : \\no Self-Attention Mechanism : Allows the model to weigh the importance of \\ndifferent words in a sentence when encoding a particular word.  \\no Encoder -Decoder Structure : Standard transformer architecture includes an \\nencoder to process the input sequence and a decoder to generate the output \\nsequence.  \\no Pre-trained Models (BERT, GPT) : Models like BERT (Bidirectional Encoder \\nRepresentations from Transformers) and GPT (Generative Pre -trained \\nTransformer) are pre -trained on large corpora and fine -tuned for specific tasks.  \\n• Strengths : Superior performance on NLP tasks, parallelizable for efficient training, \\ncaptures long -range dependencies.  \\n• Weaknesses : Requires substantial computational resources, can be difficult to interpret.  \\nTransfer Learning  \\nTransfer learning involves leveraging a pre -trained model on a new but related task, reducing the \\nneed for large amounts of labeled data.  \\nPre-trained Models  \\n• Concept : Models are pre -trained on large datasets and then fine -tuned on a smaller, task -\\nspecific dataset.  \\n• Common Use Cases : Image classification (e.g., using models like VGG, ResNet), \\nnatural language processing (e.g., using models like BERT, GPT).  ', metadata={'page': 1, 'source': 'file_3.pdf'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1719b4b-95a7-4d5f-ac45-7331146111af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Clustering  \\nClustering is a type of unsupervised learning that involves grouping a set of objects in such a \\nway that objects in the same group (or cluster) are more similar to each other than to those in \\nother groups.  \\nk-Means Clustering  \\n• Concept : k-Means is one of the simplest and most popular clustering algorithms. It \\npartitions the dataset into k clusters, where each data point belongs to the cluster with the \\nnearest mean.  \\n• Algorithm : \\n1. Initialize k cluster centroids randomly.  \\n2. Assign each data point to the nearest centroid.  \\n3. Recalculate the centroids as the mean of all points assigned to each cluster.  \\n4. Repeat steps 2 and 3 until convergence (i.e., centroids no longer change \\nsignificantly).  \\n• Strengths : Simple and fast for small to medium -sized datasets.  \\n• Weaknesses : Sensitive to initial centroid positions, may converge to a local minimum, \\nnot suitable for non -globular clusters or clusters of different sizes.  \\nHierarchical Clustering  \\n• Concept : Hierarchical clustering creates a hierarchy of clusters using either a top -down \\n(divisive) or bottom -up (agglomerative) approach.  \\n• Algorithm : \\no Agglomerative : \\n1. Start with each data point as its own cluster.  \\n2. Merge the closest pair of clusters.  \\n3. Repeat until all points are in a single cluster.  \\no Divisive : \\n1. Start with all points in one cluster.  \\n2. Recursively split clusters until each point is its own cluster.  \\n• Strengths : No need to specify the number of clusters in advance, produces a dendrogram \\nfor visualization.  \\n• Weaknesses : Computationally intensive, especially for large datasets, sensitive to noise \\nand outliers.  \\nDBSCAN (Density -Based Spatial Clustering of Applications with Noise)  \\n• Concept : DBSCAN groups together points that are closely packed and marks points that \\nare isolated in low -density regions as outliers.  \\n• Algorithm : \\n1. Define parameters ε (epsilon, the maximum radius of the neighborhood) and \\nMinPts  (minimum number of points in the neighborhood).  \\n2. Classify points as core points, reachable points, or outliers.  ', metadata={'page': 0, 'source': 'file_2.pdf'})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d335fd89-afa1-4ce8-a115-6796107a0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8a04618-71ea-414f-83b8-302bfe6ed067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is supervised learning?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c36a5e4-1a2a-4a76-bd31-851929b4c6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"MachineLearning-Lecture02  \\nInstructor (Andrew Ng) :All right, good morning, welcom e back. So before we jump \\ninto today's material, I just have one admini strative announcement, which is graders. So I \\nguess sometime next week, we'll hand out the fi rst homework assignment for this class.  \\nIs this loud enough, by the way? Can people in  the back hear me? No. Can you please \\nturn up the mic a bit louder? Is this bette r? Is this okay? This is okay? Great.  \\nSo sometime next week, we'll hand out the firs t problem sets and it'll be two weeks after \\nthat, and the way we grade homework problems in this class is by some combination of \\nTAs and graders, where graders are usually me mbers – students currently in the class.  \\nSo in maybe about a week or so, I'll email the class to solicit applica tions for those of you \\nthat might be interested in becoming graders fo r this class, and ther e's usually sort of a \\nfun thing to do. So four times this quarter, th e TAs, and the graders, and I will spend one \\nevening staying up late and grad ing all the homework problems.  \\nFor those of you who that have never taught a cla ss before, or sort of been a grader, it's an \\ninteresting way for you to see, you know, what  the other half of the teaching experience \\nis. So the students that grade for the first time sort of get to learn about what it is that \\nreally makes a difference between a good so lution and amazing solution. And to give \\neveryone to just how we do points assignments, or what is it that causes a solution to get \\nfull marks, or just how to write amazing so lutions. Becoming a grad er is usually a good \\nway to do that.  \\nGraders are paid positions and you also get free  food, and it's usually fun for us to sort of \\nhang out for an evening and grade all the a ssignments. Okay, so I will send email. So \\ndon't email me yet if you want to be a grader. I'll send email to the entire class later with \\nthe administrative details and to solicit app lications. So you can email us back then, to \\napply, if you'd be interested in being a grader.  \\nOkay, any questions about that? All right, okay, so let's get started with today's material. \\nSo welcome back to the second lecture. What  I want to do today is talk about linear \\nregression, gradient descent, and the norma l equations. And I should also say, lecture \\nnotes have been posted online and so if some  of the math I go over today, I go over rather \\nquickly, if you want to see every equation wr itten out and work through the details more \\nslowly yourself, go to the course homepage and download detailed lecture notes that \\npretty much describe all the mathematical, te chnical contents I'm going to go over today.  \\nToday, I'm also going to delve into a fair amount  – some amount of linear algebra, and so \\nif you would like to see a refres her on linear algebra, this w eek's discussion section will \\nbe taught by the TAs and will be a refresher on linear algebra. So if some of the linear \\nalgebra I talk about today sort of seems to be going by pretty quickl y, or if you just want \\nto see some of the things I'm claiming today with our proof, if you wa nt to just see some \\nof those things written out in  detail, you can come to this week's discussion section.\", metadata={'page': 0, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture02.pdf'})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9bfd738-27bb-49fc-823e-76231ccdaf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"So in supervised learning, this is what we 're going to do. We're given a training set, and \\nwe're going to feed our training set, compri sing our M training example, so 47 training \\nexamples, into a learning algorithm. Okay, and our algorithm then has output function \\nthat is by tradition, and for hist orical reasons, is usually de noted lower case alphabet H, \\nand is called a hypothesis. Don't worry too mu ch about whether the term hypothesis has a \\ndeep meaning. It's more a term that's used for historical reasons.  \\nAnd the hypothesis's job is to take this i nput. There's some new [inaudible]. What the \\nhypothesis does is it takes this  input, a new living area in s quare feet saying and output \\nestimates the price of this house. So the hypothesis H maps from inputs X to outputs Y. \\nSo in order to design a learning algorithm, the first thing we have to decide is how we \\nwant to represent the hypothesis, right.  \\nAnd just for this purposes of th is lecture, for the purposes of  our first learning algorithm, \\nI'm going to use a linear representation for the hypothesis. So I'm going to represent my \\nhypothesis as H of X equals theta zero, plus th eta 1X, where X here is  an input feature, \\nand so that's the size of the house we're considering.  \\nAnd more generally, come back to this, mo re generally for many regression problems we \\nmay have more than one input feature. So for example, if instead of  just knowing the size \\nof the houses, if we know also the number of  bedrooms in these houses, let's say, then, so \\nif our training set also has a second feature,  the number of bedrooms in the house, then \\nyou may, let's say X1 denote the size and squa re feet. Let X have script two denote the \\nnumber of bedrooms, and then I would write the hypothesis, H of X,  as theta rho plus \\ntheta 1X1 plus theta 2X2.  \\nOkay, and sometimes when I went to take the hypothesis H, and when I went to make \\nthis dependent on the theta is explicit, I'll sometimes write this as H subscript theta of X. And so this is the price that my hypothesis predicts a house with features X costs. So \\ngiven the new house with features X, a certa in size and a certain number of bedrooms, \\nthis is going to be the price that my hypothe sis predicts this house is going to cost.  \\nOne final piece of notation, so for conciseness, just to write this a bit more compactly I'm \\ngoing to take the convention of defining X0 to be equal to one, and so I can now write H \\nof X to be equal to sum from I equals one to two of theta I, oh sorr y, zero to two, theta I, \\nX I. And if you think of theta as an X, as vect ors, then this is just theta transpose X.  \\nAnd the very final piece of notation is I'm al so going to let lower case N be the number of \\nfeatures in my learning problem . And so this actually becomes a sum from I equals zero \\nto N, where in this example if you have two features, N would be equal to two.  \\nAll right, I realize that was a fair amount of  notation, and as I proceed through the rest of \\nthe lecture today, or in future weeks as well,  if some day you're looking at me write a \\nsymbol and you're wondering, gee, what was th at simple lower case N again? Or what \\nwas that lower case X again, or whatever, pleas e raise hand and I'll answer. This is a fair\", metadata={'page': 3, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture02.pdf'})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e1391f4-9527-4193-9655-50976adb2f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='classes teach. And this is something I\\'m rea lly convinced is a huge deal, and so by the \\nend of this class, I hope all of you will be master carpenters. I hope all of you will be \\nreally good at applying these learning algor ithms and getting them to work amazingly \\nwell in many problems. Okay?  \\nLet\\'s see. So [inaudible] the board. After lear ning theory, there\\'s a nother class of learning \\nalgorithms that I then want to teach you a bout, and that\\'s unsupervised learning. So you \\nrecall, right, a little ea rlier I drew an example like this , right, where you have a couple of \\nfeatures, a couple of input vari ables and sort of malignant tumors and benign tumors or \\nwhatever. And that was an example of a s upervised learning problem because the data \\nyou have gives you the right answer for each of your patients. The data tells you this \\npatient has a malignant tumor;  this patient has a benign tumor. So it had the right \\nanswers, and you wanted the algorithm to just produce more of the same.  \\nIn contrast, in an unsupervised learning problem , this is the sort of data you get, okay? \\nWhere speaking loosely, you\\'re given a data se t, and I\\'m not gonna tell you what the right \\nanswer is on any of your data. I\\'m just gonna give you a data set and I\\'m gonna say, \"Would you please find interesting structure in this data set?\" So that\\'s the unsupervised \\nlearning problem where you\\'re sort of not given the right answer for everything.  \\nSo, for example, an algorithm may find structure in the data in the form of the data being \\npartitioned into two clusters, or clustering is  sort of one example of an unsupervised \\nlearning problem.  \\nSo I hope you can see this. It turns out that th ese sort of unsupervised  learning algorithms \\nare also used in many problems. This is a scr een shot — this is a picture I got from Sue \\nEmvee, who\\'s a PhD student here, who is a pplying unsupervised learning algorithms to \\ntry to understand gene data, so is trying to  look at genes as individuals and group them \\ninto clusters based on properties of what ge nes they respond to — based on properties of \\nhow the genes respond to different experiments.  \\nAnother interesting application of [inaudible] sorts of clus tering algorithms is actually \\nimage processing, this which I got from Steve Gules, who\\'s another PhD student. It turns \\nout what you can do is if you give this sort of  data, say an image, to certain unsupervised \\nlearning algorithms, they will then learn to group pixels together and say, gee, this sort of \\npixel seems to belong together , and that sort of pixel seems to belong together.  \\nAnd so the images you see on the bottom — I guess you can just barely see them on there \\n— so the images you see on the bottom are groupings — are what the algorithm has done \\nto group certain pixels together. On a small di splay, it might be easier to just look at the \\nimage on the right. The two images on the botto m are two sort of identical visualizations \\nof the same grouping of the pixe ls into [inaudible] regions.  \\nAnd so it turns out that this sort of clustering algorithm or this sort of unsupervised \\nlearning algorithm, which learns  to group pixels together, it turns out to be useful for \\nmany applications in vision, in co mputer vision image processing.', metadata={'page': 15, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture01.pdf'})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cd3ff06-0cf2-446c-8f96-d17801cd3c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 0, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture02.pdf'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3075a93f-a31c-40f9-ae45-5040839a924e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 3, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture02.pdf'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fccaf6a-31a8-4c3d-9b00-92fe2959b1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 15,\n",
       " 'source': '/kaggle/input/cslectures/MachineLearning-Lecture01.pdf'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd4d39-5417-40c6-8f52-bf333838702e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a33c87c-6e2e-464f-9215-5254cdcd727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import Together\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = Together(\n",
    "#     model=\"META-LLAMA/LLAMA-2-7B-CHAT-HF\",\n",
    "#     together_api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\"\n",
    "# )\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=\"24cdbdf50106e08f6ba3328ac07f97a73eb440ae36da6cdd72f9b091ccca850a\",\n",
    "    model=\"META-LLAMA/LLAMA-3-8B-CHAT-HF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba744b2d-0c7c-40f5-9e43-91f59bbdfeeb",
   "metadata": {},
   "source": [
    "# Retriever Types\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030d8d5-66a7-4a89-8d23-dd41f25de6e9",
   "metadata": {},
   "source": [
    "### Self-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "10d280f7-dd50-4aa8-b15f-2b751e6d2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f7da1db8-53ca-43f7-8600-fbe52007f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"lectures on machine learning based on three pdf files: file_1.pdf, file_2.pdf, file_3.pdf\",  \n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page number within the document\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1fe956fd-878c-4345-aca4-9dba5c2fedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable stores a brief description of the content of the documents in the vectordb. \n",
    "# In this case, it's set to \"Lecture notes\", indicating that the documents are lecture notes.\n",
    "document_content_description = \"Lectures on machine learning\"\n",
    "\n",
    "# llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
    "self_query_retriever = SelfQueryRetriever.from_llm(\n",
    "    model,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "20f39a8e-372d-445b-8bba-bc4f2bde1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = self_query_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "327a5e79-6256-41e5-abeb-457f80b7fe8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b095beb6-c288-475f-87e7-cfd7026d50a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 16, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture01.pdf'}\n",
      "{'page': 1, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture01.pdf'}\n",
      "{'page': 9, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture01.pdf'}\n",
      "{'page': 5, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19f3d1fb-715b-4fb4-88d6-6eae4fe6ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Clustering is a method of unsupervised learning that involves grouping a set of objects in such a \\nway that objects in the same group (cluster) are more similar to each other than to those in other \\ngroups.  \\n• k-Means Clustering : Partitions the data into k clusters by minimizing the variance \\nwithin each cluster.  \\n• Hierarchical Clustering : Builds a hierarchy of clusters either through a bottom -up \\n(agglomerative) or top -down (divisive) approach.  \\n• DBSCAN (Density -Based Spatial Clustering of Applications with Noise) : Groups \\ntogether points that are close to each other based on a distance measurement, and marks \\npoints that are in low -density regions as outliers.  \\nDimensionality Reduction  \\nDimensionality reduction is the process of reducing the number of random variables under \\nconsideration by obtaining a set of principal variables.  \\n• Principal Component Analysis (PCA) : Projects the data into a lower -dimensional space \\nby maximizing the variance along the principal components.  \\n• t-Distributed Stochastic Neighbor Embedding (t -SNE) : Primarily used for visualizing \\nhigh-dimensional data by reducing it to two or three dimensions.  \\n• Linear Discriminant Analysis (LDA) : Finds the linear combinations of features that \\nbest separate two or more classes of objects or events.  \\nAnomaly Detection  \\nAnomaly detection aims to identify rare items, events, or observations that raise suspicions by \\ndiffering significantly from the majority of the data.  \\n• Statistical Methods : Assume a statistical distribution for the data and identify points that \\ndeviate significantly from this distribution.  \\n• Machine Learning Methods : Include clustering -based methods (e.g., DBSCAN) and \\nmodel -based approaches (e.g., autoencoders).  \\nAssociation Rules  \\nAssociation rule learning is a rule -based machine learning method for discovering interesting \\nrelations between variables in large databases.  \\n• Apriori Algorithm : Identifies frequent itemsets and then derives association rules from \\nthese itemsets.  \\n• Eclat Algorithm : Uses a depth -first search strategy to find frequent itemsets and is often \\nfaster than Apriori for large datasets.  \\n ', metadata={'page': 2, 'source': 'file_1.pdf'})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9100f-63c0-499b-bd11-58d0c5c61fc7",
   "metadata": {},
   "source": [
    "### Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b484d989-ffd4-4b63-82db-e13c63beb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9464fe85-50fc-48b9-9201-17daf0ef602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is supervised learning?\"\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3a77435-c43a-4437-8e53-e4dc3ba29501",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16c25b2f-b491-4a13-94de-fc73443b5d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d6b7e2d-ab03-43b9-a375-43811cd4cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\test\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\envs\\test\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\envs\\test\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\envs\\test\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "NO_OUTPUT. There is no relevant part of the context that is related to the question \"what is supervised learning?\". The context appears to be about administrative announcements, grading assignments, and linear regression, but does not mention supervised learning.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Here are the extracted relevant parts:\n",
      "\n",
      "So in supervised learning, this is what we 're going to do. We're given a training set, and we're going to feed our training set, compri sing our M training example, so 47 training examples, into a learning algorithm. Okay, and our algorithm then has output function that is by tradition, and for hist orical reasons, is usually de noted lower case alphabet H, and is called a hypothesis.\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c51c624-a562-48f1-8b2d-ec762d25a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture02.pdf'}\n",
      "{'page': 3, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture02.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in compressed_docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c94c2-8391-4ba2-8153-11c893fc8b3e",
   "metadata": {},
   "source": [
    "# Legacy Chains ~ retrievers\n",
    "\n",
    "### - RetrievalQA\n",
    "### - ConversationalRetrievalChain\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/chains/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed16e53-a98f-4484-b822-4bc766488aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-01 17:23:37 - Created default config file at D:\\Work\\NLP\\Langchain\\.chainlit\\config.toml\n",
      "2024-07-01 17:23:37 - Created default translation directory at D:\\Work\\NLP\\Langchain\\.chainlit\\translations\n",
      "2024-07-01 17:23:37 - Created default translation file at D:\\Work\\NLP\\Langchain\\.chainlit\\translations\\en-US.json\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "import chainlit as cl\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72671b0d-f72e-4deb-8dc1-c899aee37f94",
   "metadata": {},
   "source": [
    "ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fb781840-c4be-402e-9917-62e652170425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    model,\n",
    "    memory=memory,\n",
    "    retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    "    # retriever=self_query_retriever,\n",
    "    # retriever=compression_retriever,  \n",
    "\n",
    "    # return_source_documents=True,\n",
    "    # chain_type=\"map_reduce\"\n",
    "    # chain_type=\"refine\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "aac74530-12dc-4ce1-bebd-2556371c69fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'who is the lecturer?',\n",
       " 'chat_history': [HumanMessage(content='who is the lecturer?'),\n",
       "  AIMessage(content='The lecturer in this context is Andrew Ng, a well-known artificial intelligence researcher and entrepreneur. At the time of this recording, he was an assistant professor at Stanford University and was teaching a class on machine learning.')],\n",
       " 'answer': 'The lecturer in this context is Andrew Ng, a well-known artificial intelligence researcher and entrepreneur. At the time of this recording, he was an assistant professor at Stanford University and was teaching a class on machine learning.'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({\"question\": \"who is the lecturer?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0614671f-84f3-40b6-a23f-efe674532c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what does he teach',\n",
       " 'chat_history': [HumanMessage(content='who is the lecturer?'),\n",
       "  AIMessage(content='The lecturer in this context is Andrew Ng, a well-known artificial intelligence researcher and entrepreneur. At the time of this recording, he was an assistant professor at Stanford University and was teaching a class on machine learning.'),\n",
       "  HumanMessage(content='what does he teach'),\n",
       "  AIMessage(content=\"Andrew Ng is a well-known artificial intelligence (AI) researcher and educator. He teaches various topics related to AI, machine learning, and data science. Specifically, he has taught courses on:\\n\\n1. Machine Learning: Ng has taught machine learning courses at Stanford University, Coursera, and edX. His courses cover topics such as linear regression, logistic regression, neural networks, and deep learning.\\n2. Deep Learning: Ng has also taught deep learning courses, focusing on topics like convolutional neural networks, recurrent neural networks, and generative adversarial networks.\\n3. Artificial Intelligence: Ng has taught AI courses, covering topics such as natural language processing, computer vision, and robotics.\\n4. Data Science: Ng has taught data science courses, focusing on topics like data preprocessing, feature engineering, and data visualization.\\n\\nNg's teaching style is known for being engaging, interactive, and accessible to students with varying levels of prior knowledge in AI and machine learning. His courses often include hands-on exercises, quizzes, and projects to help students apply theoretical concepts to real-world problems.\")],\n",
       " 'answer': \"Andrew Ng is a well-known artificial intelligence (AI) researcher and educator. He teaches various topics related to AI, machine learning, and data science. Specifically, he has taught courses on:\\n\\n1. Machine Learning: Ng has taught machine learning courses at Stanford University, Coursera, and edX. His courses cover topics such as linear regression, logistic regression, neural networks, and deep learning.\\n2. Deep Learning: Ng has also taught deep learning courses, focusing on topics like convolutional neural networks, recurrent neural networks, and generative adversarial networks.\\n3. Artificial Intelligence: Ng has taught AI courses, covering topics such as natural language processing, computer vision, and robotics.\\n4. Data Science: Ng has taught data science courses, focusing on topics like data preprocessing, feature engineering, and data visualization.\\n\\nNg's teaching style is known for being engaging, interactive, and accessible to students with varying levels of prior knowledge in AI and machine learning. His courses often include hands-on exercises, quizzes, and projects to help students apply theoretical concepts to real-world problems.\"}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({\"question\": \"what does he teach\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6ca1f04e-1ac1-4a26-904b-13a8f4012811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is supervised learning',\n",
       " 'chat_history': [HumanMessage(content='who is the lecturer?'),\n",
       "  AIMessage(content='The lecturer in this context is Andrew Ng, a well-known artificial intelligence researcher and entrepreneur. At the time of this recording, he was an assistant professor at Stanford University and was teaching a class on machine learning.'),\n",
       "  HumanMessage(content='what does he teach'),\n",
       "  AIMessage(content=\"Andrew Ng is a well-known artificial intelligence (AI) researcher and educator. He teaches various topics related to AI, machine learning, and data science. Specifically, he has taught courses on:\\n\\n1. Machine Learning: Ng has taught machine learning courses at Stanford University, Coursera, and edX. His courses cover topics such as linear regression, logistic regression, neural networks, and deep learning.\\n2. Deep Learning: Ng has also taught deep learning courses, focusing on topics like convolutional neural networks, recurrent neural networks, and generative adversarial networks.\\n3. Artificial Intelligence: Ng has taught AI courses, covering topics such as natural language processing, computer vision, and robotics.\\n4. Data Science: Ng has taught data science courses, focusing on topics like data preprocessing, feature engineering, and data visualization.\\n\\nNg's teaching style is known for being engaging, interactive, and accessible to students with varying levels of prior knowledge in AI and machine learning. His courses often include hands-on exercises, quizzes, and projects to help students apply theoretical concepts to real-world problems.\"),\n",
       "  HumanMessage(content='what is supervised learning'),\n",
       "  AIMessage(content=\"According to the context, supervised learning is a type of learning where a training set is given to a learning algorithm, comprising M training examples, and the algorithm's output function, called a hypothesis, is trained to map input X to output Y. The hypothesis's job is to take an input, such as a new house's size and features, and output the predicted price of the house.\")],\n",
       " 'answer': \"According to the context, supervised learning is a type of learning where a training set is given to a learning algorithm, comprising M training examples, and the algorithm's output function, called a hypothesis, is trained to map input X to output Y. The hypothesis's job is to take an input, such as a new house's size and features, and output the predicted price of the house.\"}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({\"question\": \"what is supervised learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "70e14342-59b5-4b5c-9a1a-e65b389401a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory\n",
      "Human: who is the lecturer?\n",
      "AI: The lecturer in this context is Andrew Ng, a well-known artificial intelligence researcher and entrepreneur. At the time of this recording, he was an assistant professor at Stanford University and was teaching a class on machine learning.\n",
      "Human: what does he teach\n",
      "AI: Andrew Ng is a well-known artificial intelligence (AI) researcher and educator. He teaches various topics related to AI, machine learning, and data science. Specifically, he has taught courses on:\n",
      "\n",
      "1. Machine Learning: Ng has taught machine learning courses at Stanford University, Coursera, and edX. His courses cover topics such as linear regression, logistic regression, neural networks, and deep learning.\n",
      "2. Deep Learning: Ng has also taught deep learning courses, focusing on topics like convolutional neural networks, recurrent neural networks, and generative adversarial networks.\n",
      "3. Artificial Intelligence: Ng has taught AI courses, covering topics such as natural language processing, computer vision, and robotics.\n",
      "4. Data Science: Ng has taught data science courses, focusing on topics like data preprocessing, feature engineering, and data visualization.\n",
      "\n",
      "Ng's teaching style is known for being engaging, interactive, and accessible to students with varying levels of prior knowledge in AI and machine learning. His courses often include hands-on exercises, quizzes, and projects to help students apply theoretical concepts to real-world problems.\n",
      "Human: what is supervised learning\n",
      "AI: According to the context, supervised learning is a type of learning where a training set is given to a learning algorithm, comprising M training examples, and the algorithm's output function, called a hypothesis, is trained to map input X to output Y. The hypothesis's job is to take an input, such as a new house's size and features, and output the predicted price of the house.\n",
      "output_key\n",
      "None\n",
      "input_key\n",
      "None\n",
      "return_messages\n",
      "True\n",
      "human_prefix\n",
      "Human\n",
      "ai_prefix\n",
      "AI\n",
      "memory_key\n",
      "chat_history\n"
     ]
    }
   ],
   "source": [
    "for i in memory:\n",
    "    for j in i:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c1a721cf-5b69-4505-b113-00164f2809f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='who is the lecturer?'), AIMessage(content='The lecturer in this context is Andrew Ng, a well-known artificial intelligence researcher and entrepreneur. At the time of this recording, he was an assistant professor at Stanford University and was teaching a class on machine learning.'), HumanMessage(content='what does he teach'), AIMessage(content=\"Andrew Ng is a well-known artificial intelligence (AI) researcher and educator. He teaches various topics related to AI, machine learning, and data science. Specifically, he has taught courses on:\\n\\n1. Machine Learning: Ng has taught machine learning courses at Stanford University, Coursera, and edX. His courses cover topics such as linear regression, logistic regression, neural networks, and deep learning.\\n2. Deep Learning: Ng has also taught deep learning courses, focusing on topics like convolutional neural networks, recurrent neural networks, and generative adversarial networks.\\n3. Artificial Intelligence: Ng has taught AI courses, covering topics such as natural language processing, computer vision, and robotics.\\n4. Data Science: Ng has taught data science courses, focusing on topics like data preprocessing, feature engineering, and data visualization.\\n\\nNg's teaching style is known for being engaging, interactive, and accessible to students with varying levels of prior knowledge in AI and machine learning. His courses often include hands-on exercises, quizzes, and projects to help students apply theoretical concepts to real-world problems.\"), HumanMessage(content='what is supervised learning'), AIMessage(content=\"According to the context, supervised learning is a type of learning where a training set is given to a learning algorithm, comprising M training examples, and the algorithm's output function, called a hypothesis, is trained to map input X to output Y. The hypothesis's job is to take an input, such as a new house's size and features, and output the predicted price of the house.\")]), return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924e268-44b4-4e6f-a92f-a36f51daa472",
   "metadata": {},
   "source": [
    "RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1f9212cb-2bb0-4540-8887-5f1cf703d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "90714ceb-8044-4afa-9173-40caaf3a4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    model,\n",
    "    \n",
    "    retriever=vectordb.as_retriever(search_type = \"mmr\"),\n",
    "    # retriever=self_query_retriever,\n",
    "    # retriever=compression_retriever,  \n",
    "\n",
    "    return_source_documents=True,\n",
    "    # chain_type=\"map_reduce\"\n",
    "    # chain_type=\"refine\"\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9898355c-610b-4d1a-9acc-95be69afa813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'who are the TAs',\n",
       " 'result': 'According to the context, TAs (Teaching Assistants) are mentioned as people who will help prove some of the facts about the trace operator and derivatives in the discussion section or who can verify the proofs of all of these. Thanks for asking!',\n",
       " 'source_documents': [Document(page_content=\"And so if you – and this is a parameter of the algorithm that's often set by hand. If you \\nchoose alpha to be too small than your steep est descent algorithm will take very tiny \\nsteps and take a long time to converge. If alpha  is too large then the steepest descent may \\nactually end up overshooting the minimum, if  you're taking too a ggressive a step.  \\nYeah?  \\nStudent: [Inaudible].  \\nInstructor (Andrew Ng) :Say that again?  \\nStudent: Isn't there a one over two missing somewhere?  \\nInstructor (Andrew Ng) :Is there a one-half missing?  \\nStudent: I was [inaudible].  \\nInstructor (Andrew Ng) :Thanks. I do make lots of errors  in that. Any questions about \\nthis?  \\nAll right, so let me just wrap this property into an algorithm. So over there I derived the \\nalgorithm where you have just one training example, more generally for M training \\nexamples, gradient descent becomes the following. We're going to repeat until \\nconvergence the following step.  \\nOkay, theta I gets updated as theta I and I'm just writing out the appropriate equation for \\nM examples rather than one example. Theta I gets updated. Theta I minus alpha times the sum from I equals one to M. Okay, and I won't bother to show it, but you can go home \\nand sort of verify for yourself that this summation here, this is indeed the partial \\nderivative with respect to theta I of J of theta, where if you us e the original definition of J \\nof theta for when you have M training examples.  \\nOkay, so I'm just going to show – switch back to the laptop display. I'm going to show \\nyou what this looks like when you run the algor ithm. So it turns out that for the specific \\nproblem of linear regression, or ordinary release squares, which is what we're doing \\ntoday, the function J of theta actually does not lo ok like this nasty one that I'll show you \\njust now with a multiple local optima.  \\nIn particular, it turns out for ordi nary release squares, the functi on J of theta is – it's just a \\nquadratic function. And so we'll always ha ve a nice bow shape, like what you see up \\nhere, and only have one global mini mum with no other local optima.  \\nSo when you run gradient descent, here are actu ally the contours of the function J. So the \\ncontours of a bow shaped function like that  are going to be ellipses, and if you run \\ngradient descent on this algorithm, here's what  you might get. Let's see, so I initialize the\", metadata={'page': 8, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture02.pdf'}),\n",
       "  Document(page_content=\"I'll just show you one example, and this is a rather cool one that two students, Ashutosh \\nSaxena and Min Sun here did, wh ich is given an image like this, right? This is actually a \\npicture taken of the Stanford campus. You can apply that sort of cl ustering algorithm and \\ngroup the picture into regions. Let me actually blow that up so that you can see it more \\nclearly. Okay. So in the middle, you see the lines sort of groupi ng the image together, \\ngrouping the image into [inaudible] regions.  \\nAnd what Ashutosh and Min did was they then  applied the learning algorithm to say can \\nwe take this clustering and us e it to build a 3D model of the world? And so using the \\nclustering, they then had a lear ning algorithm try to learn what the 3D structure of the \\nworld looks like so that they could come up with a 3D model that you can sort of fly \\nthrough, okay? Although many people used to th ink it's not possible to take a single \\nimage and build a 3D model, but using a lear ning algorithm and that sort of clustering \\nalgorithm is the first step. They were able to.  \\nI'll just show you one more example. I like this  because it's a picture of Stanford with our \\nbeautiful Stanford campus. So again, taking th e same sort of clustering algorithms, taking \\nthe same sort of unsupervised learning algor ithm, you can group the pixels into different \\nregions. And using that as a pre-processing step, they eventually built this sort of 3D model of Stanford campus in a single picture.  You can sort of walk  into the ceiling, look \\naround the campus. Okay? This actually turned out to be a mix of supervised and \\nunsupervised learning, but the unsupervised lear ning, this sort of cl ustering was the first \\nstep.  \\nSo it turns out these sorts of unsupervised — clustering algorithms are actually routinely \\nused for many different problems, things like organizing computing clusters, social \\nnetwork analysis, market segmentation, so if you're a marketer and you want to divide your market into different segments or diffe rent groups of people to market to them \\nseparately; even for astronomical data an alysis and understanding how galaxies are \\nformed. These are just a sort of small sample  of the applications of unsupervised learning \\nalgorithms and clustering algorithms that we 'll talk about later in this class.  \\nJust one particularly cool example of an uns upervised learning algorithm that I want to \\ntell you about. And to motivate that, I'm gonna  tell you about what's called the cocktail \\nparty problem, which is imagine that you're at  some cocktail party a nd there are lots of \\npeople standing all over. And you know how it is, right, if you're at a large party, \\neveryone's talking, it can be sometimes very hard  to hear even the person in front of you. \\nSo imagine a large cocktail party with lots of people. So the problem is, is that all of these people talking, can you separate out the voice of just the person you're interested in \\ntalking to with all this  loud background noise?  \\nSo I'll show you a specific example in a second, but here's a cocktail party that's I guess \\nrather sparsely attended by just two people.  But what we're gonna do is we'll put two \\nmicrophones in the room, okay? And so becau se the microphones are just at slightly \\ndifferent distances to the two people, and th e two people may speak in slightly different \\nvolumes, each microphone will pick up an overl apping combination of these two people's\", metadata={'page': 16, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture01.pdf'}),\n",
       "  Document(page_content=\"theta – notice there's no subscript I now – updating the parameter vector theta as the \\nprevious parameter minus alpha times the gradient.  \\nOkay, and so in this equation all of these quanti ties, theta, and this gradient vector, all of \\nthese are n plus one dimensional vectors. I was using the boards out of order, wasn't I? So \\nmore generally, if you have a function F that maps from the space of matrices A, that \\nmaps from, say, the space of N by N matrices  to the space of real numbers. So if you \\nhave a function, F of A, wher e A is an N by N matrix.  \\nSo this function is matched from matrices to real numbers, the function that takes this \\ninput to matrix. Let me define the derivative w ith respect to F of the matrix A. Now, I'm \\njust taking the gradient of F with respect to  its input, which is the matrix. I'm going to \\ndefine this itself to be a matrix.  \\nOkay, so the derivative of F with respect to A is itself a matrix, and the matrix contains \\nall the partial derivatives of F with respect to the elements of  A. One more definition is if \\nA is a square matrix, so if A is an n by n matrix, number of rows  equals number of \\ncolumns, let me define the trace of A to be equal to the sum of A' s diagonal elements. So \\nthis is just sum over I of A, I, I.  \\nFor those of you that haven't seen this sort  of operator notation be fore, you can think of \\ntrace of A as the trace operator applied to the square matrix A, but it's more commonly \\nwritten without the parentheses. So I usually wr ite trace of A like this, and this just means \\nthe sum of diagonal elements.  \\nSo here are some facts about the trace operator and about derivatives, and I'm just going \\nto write these without proof. You can also have the TAs prove some of them in the \\ndiscussion section, or you can actually go home and verify the proofs of all of these.  \\nIt turns out that given two matr ices, A and B, the trace of the matrix A times B is equal to \\nthe trace of B, A. Okay, I'm not going to pr ove this, but you should be able to go home \\nand prove this yourself without too much di fficulty. And similarly, the trace of a product \\nof three matrices, so if you can take the matrix  at the end and cycli cally permeate it to the \\nfront.  \\nSo trace of A times B, times C, is equal to the trace of C, A, B. So take the matrix C at \\nthe back and move it to the front, and this is also equal to the trace of B, C. Take the \\nmatrix B and move it to the front.  \\nOkay, also, suppose you have a function F of A wh ich is defined as a trace of A, B. Okay, \\nso this is, right, the trace is a real number. So  the trace of A, B is a function that takes this \\ninput of matrix A and output a real number. So then the de rivative with respect to the \\nmatrix A of this function of trace A, B, is  going to be B transposed. And this is just \\nanother fact that you can prove  by yourself by going back and referring to the definitions \\nof traces and matrix derivatives. I'm not goi ng to prove it. You should work it out.\", metadata={'page': 13, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture02.pdf'}),\n",
       "  Document(page_content='joys of machine learning firs thand and really try to think about doing a publishable piece \\nof work.  \\nSo many students will try to build a cool machine learning application. That\\'s probably \\nthe most common project. Some students will try to improve state-of-the-art machine \\nlearning. Some of those projects are also very  successful. It\\'s a littl e bit harder to do. And \\nthere\\'s also a smaller minority of students th at will sometimes try to prove — develop the \\ntheory of machine learning further or try to  prove theorems about machine learning. So \\nthey\\'re usually great projects of all of those types with applications and machine learning \\nbeing the most common. Anything else? Okay, cool.  \\nSo that was it for logistics. Let\\'s talk about  learning algorithms. So can I have the laptop \\ndisplay, please, or the projector? Actually, co uld you lower the big sc reen? Cool. This is \\namazing customer service. Thank you. I see. Okay, cool. Okay. No, that\\'s fine. I see. \\nOkay. That\\'s cool. Thanks. Okay.  \\nBig screen isn\\'t working toda y, but I hope you can read things  on the smaller screens out \\nthere. Actually, [inaudible] I think this room just got a new projector that — someone \\nsent you an excited email — was it just on Frid ay? — saying we just got a new projector \\nand they said 4,000-to-1 something or othe r brightness ratio. I don\\'t know. Someone was \\nvery excited about the new projector in this room, but I guess we\\'ll see that in operation \\non Wednesday.  \\nSo start by talking about what machine learni ng is. What is machine learning? Actually, \\ncan you read the text out there? Raise your hand if the text on the small screens is legible. \\nOh, okay, cool, mostly legible. Okay. So I\\'ll just read it out.  \\nSo what is machine learning? Way back in  about 1959, Arthur Samuel defined machine \\nlearning informally as the [inaudible] that gives computers to learn — [inaudible] that \\ngives computers the ability to learn without  being explicitly programmed. So Arthur \\nSamuel, so way back in the history of m achine learning, actually did something very \\ncool, which was he wrote a checkers progr am, which would play games of checkers \\nagainst itself.  \\nAnd so because a computer can play thousands  of games against itself relatively quickly, \\nArthur Samuel had his program play thousands  of games against itself, and over time it \\nwould start to learn to rec ognize patterns which led to wi ns and patterns which led to \\nlosses. So over time it learned things like that , \"Gee, if I get a lot of pieces taken by the \\nopponent, then I\\'m more likely to lose than win,\" or, \"Gee, if I get my pieces into a \\ncertain position, then I\\'m especially li kely to win rather than lose.\"  \\nAnd so over time, Arthur Samuel had a check ers program that woul d actually learn to \\nplay checkers by learning what are the sort of  board positions that tend to be associated \\nwith wins and what are the boa rd positions that tend to be associated with losses. And \\nway back around 1959, the amazing thing about this was that his program actually \\nlearned to play checkers much better than Arthur Samuel  himself could.', metadata={'page': 10, 'source': '/kaggle/input/cslectures/MachineLearning-Lecture01.pdf'})]}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"who are the TAs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98938b60-5104-4005-bf6c-4c5829c286e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
